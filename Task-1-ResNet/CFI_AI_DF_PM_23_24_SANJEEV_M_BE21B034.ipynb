{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVmsGWUFd-tP"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMJ4av3_YGjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf0cc06-b00b-4b83-dec2-122003efd0ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUT0FSfTbvoc"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip3 install pyprind\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3APxWObeA_c"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "* Clone the notebook to your drive. \n",
        "* The notebook has to be submitted with the subject as \"CFI_AI_DF_PM_23-24_\\<your name\\>_\\<roll number\\>\". \n",
        "\n",
        "* If you have any queries, you can reach out to the core team:\n",
        "\n",
        "| Name | Phone Number |\n",
        "| :-- | :-- |\n",
        "| Karthick Krishna | 7338857571|\n",
        "| Tharun Anand | 7904225519 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0ojuzzfedsb"
      },
      "source": [
        "# Coding Questions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0zS6tPrVsLr"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "Implement, from scratch, CNN architecture commonly used for [Image Classification](https://www.thinkautomation.com/eli5/eli5-what-is-image-classification-in-deep-learning/), namely, [VGG19](https://medium.com/mlearning-ai/image-detection-using-convolutional-neural-networks-89c9e21fffa3), [LeNet](https://towardsdatascience.com/understanding-and-implementing-lenet-5-cnn-architecture-deep-learning-a2d531ebc342), [AlexNet](https://towardsdatascience.com/alexnet-the-architecture-that-challenged-cnns-e406d5297951), [ResNet](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035), etc., using [PyTorch](https://pytorch.org/) to classify the images of the CIFAR10 dataset. \n",
        "\n",
        "**Hint:** You have to implement the `torch.nn.module` class for the models. (Brownie Points for implementing all the named networks.)\n",
        "\n",
        "**Caution:** Do not copy pre-existing implementations blindly. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fij_DJiQY5JE"
      },
      "source": [
        "A template has been provided to help you. \n",
        "\n",
        "* You may or may not edit the `None` fields. There is no need to change anything else.\n",
        "* You might or might not have to add additional lines other than what is provided. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvfFGB6UbB8Z"
      },
      "outputs": [],
      "source": [
        "# Downloading and Preparing the Dataset\n",
        "\n",
        "!gdown --id 1oYnD7Izl3LVVzjEMyLxLklX30TKWHgGG\n",
        "!unzip /content/cifar-10.zip\n",
        "!rm -rf /content/cifar-10.zip\n",
        "!mv /content/cifar-10/sample_submission.csv /content/cifar-10/test_labels.csv\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdMFciI7WQkF"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import pandas\n",
        "import numpy\n",
        "from sklearn import preprocessing\n",
        "import matplotlib\n",
        "\n",
        "import os\n",
        "import pyprind\n",
        "\n",
        "PATH = \"https://drive.google.com/drive/folders/1C4n9hGzzxiypA4s6p2NOZK4_JkAfPw43\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj-_3MeLVuz-"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.transforms import Normalize\n",
        "\n",
        "class CreateDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, mode='train'):\n",
        "        self.root_dir = root_dir\n",
        "        self.mode = mode\n",
        "\n",
        "        self.entry = pandas.read_csv(os.path.join(self.root_dir, f'{self.mode}_labels.csv'))\n",
        "        self.encoder = self._process_()\n",
        "        self.entry['label'] = self.encoder.transform(self.entry['label'])\n",
        "\n",
        "        self.transform = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def _process_(self):\n",
        "        data = pandas.read_csv(os.path.join(self.root_dir, f'{self.mode}_labels.csv'))\n",
        "        encoder = preprocessing.LabelEncoder()\n",
        "        encoder.fit(data['label'])\n",
        "        return encoder\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = self.entry.loc[index, 'id']\n",
        "        image = Image.open(os.path.join(self.root_dir, f'{self.mode}', f'{data}.png')) \n",
        "        image = self.transform(image)\n",
        "        label = self.entry.loc[index, 'label'] \n",
        "        return image, label\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1i_bmaUl9bc"
      },
      "source": [
        "#CNN Architectures\n",
        "\n",
        "CNN architectures are all some of the Computer vision neural networks generated by several top organisations while participating in the ImageNet competitions.These architectures are each developed with some innovative treat which helps us to generate more and more accurate results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsnCEEyXmI1O"
      },
      "source": [
        "##ResNet-\n",
        "Residual Network (ResNet) is a Convolutional Neural Network (CNN) architecture that overcame the “vanishing gradient” problem, making it possible to construct networks with up to thousands of convolutional layers, which outperform shallower networks.We are achieving this by adding a common Residual layer in evry ResNet layer so that the gradient can easily pass through.\n",
        "\n",
        "1. The below is the Residual Block or the repetetive block which repoeats in every layer of the ResNet layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "873VrAED5r30"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(torch.nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = torch.nn.Conv2d(out_channels, self.expansion*out_channels, kernel_size=1, bias=False)\n",
        "        self.bn3 = torch.nn.BatchNorm2d(self.expansion*out_channels)\n",
        "\n",
        "        self.shortcut = torch.nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion*out_channels:\n",
        "            self.shortcut = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels, self.expansion*out_channels,kernel_size=1, stride=stride, bias=False),\n",
        "                torch.nn.BatchNorm2d(self.expansion*out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6buC7PS1eQK"
      },
      "source": [
        "2. Given below is the ResNet Layer which is developed with the Residual block as one of the added attachments in every layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhpnlVPlWfVN"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class ResNet(torch.nn.Module):\n",
        "    def __init__(self, ResidualBlock, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(64)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.layer1 = self._make_layer(ResidualBlock, 64, 3, stride=1)\n",
        "        self.layer2 = self._make_layer(ResidualBlock, 128, 4, stride=2)\n",
        "        self.layer3 = self._make_layer(ResidualBlock, 256, 6, stride=2)\n",
        "        self.layer4 = self._make_layer(ResidualBlock, 512, 3, stride=2)\n",
        "        self.linear = torch.nn.Linear(512*ResidualBlock.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, ResidualBlock, out_channels,num_blocks ,stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * ResidualBlock.expansion\n",
        "        return torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqXhXBWr_W_r"
      },
      "outputs": [],
      "source": [
        "resnet = ResNet(ResidualBlock)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuX7NMGYqgIq"
      },
      "source": [
        "##VGGNet-\n",
        "VGG19 is an advanced CNN with pre-trained layers and a great understanding of what defines an image in terms of shape, color, and structure. VGG19 is very deep and has been trained on millions of diverse images with complex classification tasks.Below VGG19 has several layers whixh are in a pattern where there is a repetition of 3 layers-\n",
        "1. `Conv2d`-Convolutional layer\n",
        "2. `BatchNorm`- Batch Normalisation layer which brings the matrices within range when deviated\n",
        "3. `ReLU` - Rectified Linear Unit Layer\n",
        "\n",
        "Another is repetition of `Maxpooling` Layer - which is generally used for reducing the dimension of the feature maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NijXO6iq7Ne"
      },
      "outputs": [],
      "source": [
        "VGG19 = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "\n",
        "class VGG(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(VGG19)\n",
        "        self.classifier = torch.nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, vgglist):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in vgglist:\n",
        "            if x == 'M':\n",
        "                layers += [torch.nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [torch.nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           torch.nn.BatchNorm2d(x),\n",
        "                           torch.nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [torch.nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return torch.nn.Sequential(layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tClLCuPdvzly"
      },
      "source": [
        "##AlexNet-\n",
        "AlexNet. The architecture consists of eight layers: five convolutional layers and three fully-connected layers and also a flatten layer.\n",
        "\n",
        "1. `ReLU Nonlinearity`- AlexNet uses Rectified Linear Units (ReLU) instead of the tanh function, which was standard at the time.\n",
        "2. `Multiple GPUs`- Back in the day, GPUs were still rolling around with 3 gigabytes of memory (nowadays those kinds of memory would be rookie numbers).\n",
        "3. `Overlapping Pooling`- CNNs traditionally “pool” outputs of neighboring groups of neurons with no overlapping. However, when the authors introduced overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IG5_j0Xvw6e"
      },
      "outputs": [],
      "source": [
        "class AlexNet(torch.nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            torch.nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            torch.nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = torch.nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Dropout(),\n",
        "            torch.nn.Linear(256 * 6 * 6, 4096),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Dropout(),\n",
        "            torch.nn.Linear(4096, 4096),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            torch.nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9002iXVpw0j_"
      },
      "source": [
        "##LeNet-\n",
        "LeNet-5 CNN architecture is made up of 7 layers. The layer composition consists of 3 convolutional layers, 2 subsampling layers and 2 fully connected layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S71ifoQ3wz4R"
      },
      "outputs": [],
      "source": [
        "class LeNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(3, 6, kernel_size=5)\n",
        "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = torch.nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1   = torch.nn.Linear(16*5*5, 120)\n",
        "        self.fc2   = torch.nn.Linear(120, 84)\n",
        "        self.fc3   = torch.nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = F.relu(self.conv2(out))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbfHvQf90Q5T"
      },
      "source": [
        "#Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYHk4v5-WrfF"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "    def __init__(self, data, model):\n",
        "        self.data = data\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.trainloader, self.validloader, self.testloader = self.get_iterator(self.data)\n",
        "        \n",
        "        self.model = self.get_model(model).to(self.device)\n",
        "        self.criterion = self.get_criterion().to(self.device)\n",
        "        self.optimizer = self.get_optimizer()\n",
        "        self.train_loss = []\n",
        "        self.train_metrics = []\n",
        "        self.valid_loss = []\n",
        "        self.valid_metrics = []\n",
        "        self.epochs = 10\n",
        "\n",
        "    def get_iterator(self, data):\n",
        "        train, valid, test = data\n",
        "        trainloader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True, drop_last=True) \n",
        "        validloader = torch.utils.data.DataLoader(valid, batch_size=128, shuffle=False, drop_last=True) \n",
        "        testloader = torch.utils.data.DataLoader(test, batch_size=128, shuffle=False) \n",
        "        return trainloader, validloader, testloader\n",
        "\n",
        "    def get_criterion(self):\n",
        "        return torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "    def get_optimizer(self):\n",
        "        return torch.optim.RMSprop(self.model.parameters(), lr = 0.0001)\n",
        "\n",
        "    def get_model(self, model):\n",
        "        return model\n",
        "\n",
        "    def save(self, epoch):\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            }, os.path.join(PATH, \"model.pth\"))\n",
        "        \n",
        "    def load(self):\n",
        "        if os.path.exists(os.path.join(PATH, \"model.pth\")):\n",
        "            checkpoints = torch.load(os.path.join(self.args.checkpoint, \"model.pth\"), map_location=self.device)\n",
        "            self.model.load_state_dict(checkpoints['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoints['optimizer_state_dict'])\n",
        "\n",
        "    def train(self):\n",
        "        epoch_loss = 0\n",
        "        epoch_metrics = {}\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            bar = pyprind.ProgBar(len(self.trainloader), bar_char='█')\n",
        "            for index, (image, label) in enumerate(self.trainloader):\n",
        "                image = image.to(self.device)\n",
        "                label = label.to(self.device)\n",
        "                image = image.squeeze(0)\n",
        "\n",
        "                self.optimizer.zero_grad()                \n",
        "                output = self.model.forward(torch.tensor(image.clone().detach()))\n",
        "                #Evaluating the loss function\n",
        "                loss = self.criterion(output,label)\n",
        "                loss.backward()\n",
        "                epoch_loss += loss.item()\n",
        "                #Back propagation - gradient descent\n",
        "                self.optimizer.step()\n",
        "                bar.update()\n",
        "\n",
        "            epoch_loss = epoch_loss/128 #Dividing by the batch size\n",
        "\n",
        "        return epoch_loss, epoch_metrics\n",
        "\n",
        "    def evaluate(self):\n",
        "        epoch_loss = 0\n",
        "        epoch_metrics = {}\n",
        "\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            bar = pyprind.ProgBar(len(self.validloader), bar_char='█')\n",
        "            for index, (image, label) in enumerate(self.validloader):\n",
        "                image = image.to(self.device)\n",
        "                label = label.to(self.device)                \n",
        "                output = self.model(torch.tensor(image.squeeze(0).clone().detach()))\n",
        "                loss = self.criterion(output, label)\n",
        "                epoch_loss += loss.item()\n",
        "                bar.update()\n",
        "\n",
        "        return epoch_loss, epoch_metrics\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        outputs = torch.empty([0,]).to(self.device)\n",
        "\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            bar = pyprind.ProgBar(len(self.testloader), bar_char='█')\n",
        "            for index, (image, label) in enumerate(self.testloader):\n",
        "                image = image.to(self.device)\n",
        "                label = label.to(self.device)\n",
        "                output = self.model(torch.tensor(image.squeeze(0).clone().detach())).to(self.device)\n",
        "                outputs = torch.cat([outputs, output]).to(self.device)\n",
        "\n",
        "                bar.update()\n",
        "\n",
        "        return outputs\n",
        "    \n",
        "    def fit(self):\n",
        "\n",
        "        for epoch in range(1, self.epochs+1, 1):\n",
        "\n",
        "            epoch_train_loss, epoch_train_metrics = self.train()\n",
        "\n",
        "            self.train_loss.append(epoch_train_loss)\n",
        "            self.train_metrics.append(epoch_train_metrics)\n",
        "\n",
        "            epoch_valid_loss, epoch_valid_metrics = self.evaluate()\n",
        "            \n",
        "            self.valid_loss.append(epoch_valid_loss)\n",
        "            self.valid_metrics.append(epoch_valid_metrics) \n",
        "\n",
        "            print(f'Epoch {epoch}/{self.epochs+1}: Train Loss = {epoch_train_loss} | Validation Loss = {epoch_valid_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2VnYXVYYaqF",
        "outputId": "3e9236b6-2c4b-4f1a-c72a-dd322ec6d720"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-049971e97a8f>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  output = self.model.forward(torch.tensor(image.clone().detach()))\n",
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:03:06\n",
            "<ipython-input-12-049971e97a8f>:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  output = self.model(torch.tensor(image.squeeze(0).clone().detach()))\n",
            "0% [██████████████████████████████] 100% | ETA: 00:00:00"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11: Train Loss = 4.0931671699509025 | Validation Loss = 47.18881332874298\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Total time elapsed: 00:00:07\n",
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:03:03\n",
            "0% [██████████████████████████████] 100% | ETA: 00:00:00"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/11: Train Loss = 2.7422315306030214 | Validation Loss = 36.314328372478485\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Total time elapsed: 00:00:07\n",
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:03:03\n",
            "0% [██████████████████████████████] 100% | ETA: 00:00:00"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/11: Train Loss = 1.9850058257579803 | Validation Loss = 29.27767300605774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Total time elapsed: 00:00:07\n",
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:03:03\n",
            "0% [██████████████████████████████] 100% | ETA: 00:00:00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/11: Train Loss = 1.4095873963087797 | Validation Loss = 27.560781240463257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Total time elapsed: 00:00:07\n",
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:03:01\n",
            "0% [██████████████████████████████] 100% | ETA: 00:00:00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/11: Train Loss = 0.9421912017278373 | Validation Loss = 29.563781440258026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Total time elapsed: 00:00:07\n",
            "0% [█                             ] 100% | ETA: 00:02:52"
          ]
        }
      ],
      "source": [
        "train_data = CreateDataset(root_dir=\"/content/cifar-10\", mode='train') \n",
        "train_data, valid_data = torch.utils.data.random_split(train_data, [len(train_data)-len(train_data)//10, len(train_data)//10])\n",
        "test_data = CreateDataset(root_dir=\"/content/cifar-10\", mode='test') \n",
        "data = (train_data, valid_data, test_data)\n",
        "\n",
        "trainer = Trainer(data, resnet)\n",
        "trainer.fit()\n",
        "\n",
        "outputs = trainer.test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRMaE3i8_oM8"
      },
      "source": [
        "##Question 2 (Optional)\n",
        "\n",
        "EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all depth/width/resolution dimensions using a compound coefficient. Use [Transfer Learning](https://machinelearningmastery.com/transfer-learning-for-deep-learning/) to extract the layers of EfficientNet and perform image classification for the same dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPMMUZiLAZQd"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPnlJT88BETe"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip3 install pyprind\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLm2flzJAjhe"
      },
      "outputs": [],
      "source": [
        "!pip install efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU1ffsqTAkCC"
      },
      "outputs": [],
      "source": [
        "from efficientnet_pytorch import EfficientNet "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnLTcg8Vjb6o"
      },
      "outputs": [],
      "source": [
        "# Load the EfficientNet model\n",
        "enet = EfficientNet.from_pretrained('efficientnet-b0', num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud_EmnAaBLxY"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import pandas\n",
        "import numpy\n",
        "from sklearn import preprocessing\n",
        "import matplotlib\n",
        "\n",
        "import os\n",
        "import pyprind\n",
        "\n",
        "PATH = \"https://drive.google.com/drive/folders/1C4n9hGzzxiypA4s6p2NOZK4_JkAfPw43\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6lSAAcjAptE"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.transforms import Normalize\n",
        "\n",
        "class CreateDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, mode='train'):\n",
        "        self.root_dir = root_dir\n",
        "        self.mode = mode\n",
        "\n",
        "        self.entry = pandas.read_csv(os.path.join(self.root_dir, f'{self.mode}_labels.csv'))\n",
        "        self.encoder = self._process_()\n",
        "        self.entry['label'] = self.encoder.transform(self.entry['label'])\n",
        "\n",
        "        self.transform = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def _process_(self):\n",
        "        data = pandas.read_csv(os.path.join(self.root_dir, f'{self.mode}_labels.csv'))\n",
        "        encoder = preprocessing.LabelEncoder()\n",
        "        encoder.fit(data['label'])\n",
        "        return encoder\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = self.entry.loc[index, 'id']\n",
        "        image = Image.open(os.path.join(self.root_dir, f'{self.mode}', f'{data}.png')) \n",
        "        image = self.transform(image)\n",
        "        label = self.entry.loc[index, 'label'] \n",
        "        return image, label\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entry)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAuafY1198Ma"
      },
      "outputs": [],
      "source": [
        "for param in enet.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iPIZ3ji-Hv7"
      },
      "outputs": [],
      "source": [
        "enet.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Linear(1280, 512),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(p=0.5),\n",
        "    torch.nn.Linear(512, 10)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKrPW4bMAuBx"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "    def __init__(self, data, model):\n",
        "        self.data = data\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.trainloader, self.validloader, self.testloader = self.get_iterator(self.data)\n",
        "        \n",
        "        self.model = self.get_model(model).to(self.device)\n",
        "        self.criterion = self.get_criterion().to(self.device)\n",
        "        self.optimizer = self.get_optimizer()\n",
        "\n",
        "        self.train_loss = []\n",
        "        self.train_metrics = []\n",
        "        self.valid_loss = []\n",
        "        self.valid_metrics = []\n",
        "\n",
        "        self.epochs = 1\n",
        "\n",
        "    def get_iterator(self, data):\n",
        "        train, valid, test = data\n",
        "        trainloader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True, drop_last=True) \n",
        "        validloader = torch.utils.data.DataLoader(valid, batch_size=128, shuffle=False, drop_last=True) \n",
        "        testloader = torch.utils.data.DataLoader(test, batch_size=128, shuffle=False) \n",
        "        return trainloader, validloader, testloader\n",
        "\n",
        "    def get_criterion(self):\n",
        "        return torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "    def get_optimizer(self):\n",
        "        return torch.optim.Adam(self.model.parameters())\n",
        "\n",
        "    def get_model(self, model):\n",
        "        return model\n",
        "\n",
        "    def save(self, epoch):\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            }, os.path.join(PATH, \"model.pth\"))\n",
        "        \n",
        "    def load(self):\n",
        "        if os.path.exists(os.path.join(PATH, \"model.pth\")):\n",
        "            checkpoints = torch.load(os.path.join(self.args.checkpoint, \"model.pth\"), map_location=self.device)\n",
        "            self.model.load_state_dict(checkpoints['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoints['optimizer_state_dict'])\n",
        "\n",
        "    def train(self):\n",
        "        epoch_loss = 0\n",
        "        epoch_metrics = {}\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            bar = pyprind.ProgBar(len(self.trainloader), bar_char='█')\n",
        "            for index, (image, label) in enumerate(self.trainloader):\n",
        "                image = image.to(self.device)\n",
        "                label = label.to(self.device)\n",
        "                image = image.squeeze(0)\n",
        "\n",
        "                self.optimizer.zero_grad()                \n",
        "                output = self.model.forward(torch.tensor(image.clone().detach()))\n",
        "                #Evaluating the loss function\n",
        "                loss = self.criterion(output,label)\n",
        "                epoch_loss += loss.item()\n",
        "                #Back propagation - gradient descent\n",
        "                self.optimizer.step()\n",
        "                bar.update()\n",
        "\n",
        "        return epoch_loss, epoch_metrics\n",
        "\n",
        "    def evaluate(self):\n",
        "        epoch_loss = 0\n",
        "        epoch_metrics = {}\n",
        "\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            bar = pyprind.ProgBar(len(self.validloader), bar_char='█')\n",
        "            for index, (image, label) in enumerate(self.validloader):\n",
        "                image = image.to(self.device)\n",
        "                label = label.to(self.device)                \n",
        "                output = self.model(torch.tensor(image.squeeze(0).clone().detach()))\n",
        "                loss = self.criterion(output, label)\n",
        "                epoch_loss += loss.item()\n",
        "                bar.update()\n",
        "\n",
        "        return epoch_loss, epoch_metrics\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        outputs = torch.empty([0,]).to(self.device)\n",
        "\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            bar = pyprind.ProgBar(len(self.testloader), bar_char='█')\n",
        "            for index, (image, label) in enumerate(self.testloader):\n",
        "                image = image.to(self.device)\n",
        "                label = label.to(self.device)\n",
        "                output = self.model(torch.tensor(image.squeeze(0).clone().detach())).to(self.device)\n",
        "                outputs = torch.cat(torch.tensor(outputs), output).to(self.device)\n",
        "\n",
        "                bar.update()\n",
        "\n",
        "        return outputs\n",
        "    \n",
        "    def fit(self):\n",
        "\n",
        "        for epoch in range(1, self.epochs+1, 1):\n",
        "\n",
        "            epoch_train_loss, epoch_train_metrics = self.train()\n",
        "\n",
        "            self.train_loss.append(epoch_train_loss)\n",
        "            self.train_metrics.append(epoch_train_metrics)\n",
        "\n",
        "            epoch_valid_loss, epoch_valid_metrics = self.evaluate()\n",
        "            \n",
        "            self.valid_loss.append(epoch_valid_loss)\n",
        "            self.valid_metrics.append(epoch_valid_metrics) \n",
        "\n",
        "            print(f'Epoch {epoch}/{self.epochs+1}: Train Loss = {epoch_train_loss} | Validation Loss = {epoch_valid_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xt0KNHJp-X7K"
      },
      "outputs": [],
      "source": [
        "train_data = CreateDataset(root_dir=\"/content/cifar-10\", mode='train') \n",
        "train_data, valid_data = torch.utils.data.random_split(train_data, [len(train_data)-len(train_data)//10, len(train_data)//10])\n",
        "test_data = CreateDataset(root_dir=\"/content/cifar-10\", mode='test') \n",
        "data = (train_data, valid_data, test_data)\n",
        "\n",
        "trainer = Trainer(data, enet)\n",
        "trainer.fit()\n",
        "\n",
        "outputs = trainer.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gO_ZoDaDARJO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}